apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: vllm
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
          - name: vllm-leader
            image: vllm/vllm-openai:latest
            env:
              - name: HF_HUB_ENABLE_HF_TRANSFER
                value: "1"
              - name: HF_HOME
                value: "/local/huggingface"
              - name: HF_HUB_VERBOSITY
                value: "debug"
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
              - name: NCCL_DEBUG
                value: "TRACE"
              - name: MODEL_REPO
                value: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            command: ["/bin/bash"]
            args:
              - "-c"
              - |
                set -x
                wget https://raw.githubusercontent.com/kubernetes-sigs/lws/main/docs/examples/vllm/build/ray_init.sh -O /ray_init.sh; 
                chmod +x /ray_init.sh;
                /ray_init.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);
                pip install huggingface_hub
                huggingface-cli download ${MODEL_REPO}
                vllm serve ${MODEL_REPO} \
                  --host 0.0.0.0 \
                  --port 8000 \
                  --trust-remote-code \
                  --tensor-parallel-size 4 \
                  --pipeline_parallel_size 2 \
                  --max-model-len 32768 \
                  --gpu_memory_utilization 0.97 \
                  --dtype=half
            resources:
              limits:
                nvidia.com/gpu: "4"
                memory: 64Gi
                ephemeral-storage: 200Gi
              requests:
                cpu: 12
                ephemeral-storage: 200Gi
            ports:
              - containerPort: 8080
            readinessProbe:
              tcpSocket:
                port: 8080
              initialDelaySeconds: 15
              periodSeconds: 10
            volumeMounts:
              - name: local-storage
                mountPath: /local
              - name: shm
                mountPath: /dev/shm
        volumes:
        - name: local-storage
          hostPath:
            path: /root/local
            type: DirectoryOrCreate
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            image: vllm/vllm-openai:latest
            env:
              - name: HF_HUB_ENABLE_HF_TRANSFER
                value: "1"
              - name: HF_HOME
                value: "/local/huggingface"
              - name: HF_HUB_VERBOSITY
                value: "debug"
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
              - name: NCCL_DEBUG
                value: "TRACE"
              - name: MODEL_REPO
                value: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            command: ["/bin/bash"]
            args:
              - "-c"
              - |
                set -x
                wget https://raw.githubusercontent.com/kubernetes-sigs/lws/main/docs/examples/vllm/build/ray_init.sh -O /ray_init.sh; 
                chmod +x /ray_init.sh;
                 /ray_init.sh worker --ray_address=$(LWS_LEADER_ADDRESS)
            resources:
              limits:
                nvidia.com/gpu: "4"
                memory: 64Gi
                ephemeral-storage: 200Gi
              requests:
                cpu: 12
                ephemeral-storage: 200Gi
            volumeMounts:
              - name: local-storage
                mountPath: /local
              - name: shm
                mountPath: /dev/shm
        volumes:
        - name: local-storage
          hostPath:
            path: /root/local
            type: DirectoryOrCreate
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-leader
spec:
  ports:
    - name: port-8000
      port: 8000
      targetPort: 8000
  type: ClusterIP
  selector:
    leaderworkerset.sigs.k8s.io/name: vllm
    role: leader
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token-secret
type: Opaque
data:
  token: "aGZfTUVzd1BVRlVZc2ZVTkRRRkVzTXF0UVBvbUx1bXZoVmdXQg=="
