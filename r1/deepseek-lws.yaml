apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: vllm
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 4
    restartPolicy: RecreateGroupOnPodRestart
    leaderTemplate:
      metadata:
        labels:
          role: leader
      spec:
        containers:
          - name: vllm-leader
            image: ghcr.io/dims/skunkworks/vllm:latest
            env:
              - name: HF_HUB_ENABLE_HF_TRANSFER
                value: "1"
              - name: HF_HOME
                value: "/local/huggingface"
              - name: HF_HUB_VERBOSITY
                value: "debug"
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
              - name: NCCL_DEBUG
                value: "TRACE"
              - name: MODEL_REPO
                value: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
            command: ["/bin/bash"]
            args:
              - "-c"
              - |
                set -x

                # Switch on the ray dashboard
                sed -i 's/ray start --head/ray start --head --dashboard-host=0.0.0.0/' /ray_init.sh

                # start ray leader
                /ray_init.sh leader --ray_cluster_size=$(LWS_GROUP_SIZE);

                # download and install model
                huggingface-cli download ${MODEL_REPO}

                # start vllm server
                vllm serve ${MODEL_REPO} \
                  --host 0.0.0.0 \
                  --port 8000 \
                  --trust-remote-code \
                  --tensor-parallel-size 16 \
                  --pipeline-parallel-size 1 \
                  --gpu_memory_utilization 0.97 \
                  --dtype=half
            resources:
              limits:
                nvidia.com/gpu: "4"
                memory: 64Gi
                ephemeral-storage: 200Gi
                vpc.amazonaws.com/efa: 1
              requests:
                cpu: 12
                ephemeral-storage: 200Gi
                vpc.amazonaws.com/efa: 1
            ports:
              - containerPort: 8000
            readinessProbe:
              tcpSocket:
                port: 8000
              initialDelaySeconds: 15
              periodSeconds: 10
            volumeMounts:
              - name: local-storage
                mountPath: /local
              - name: shm
                mountPath: /dev/shm
        volumes:
        - name: local-storage
          hostPath:
            path: /root/local
            type: DirectoryOrCreate
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
    workerTemplate:
      spec:
        containers:
          - name: vllm-worker
            image: ghcr.io/dims/skunkworks/vllm:latest
            env:
              - name: HF_HUB_ENABLE_HF_TRANSFER
                value: "1"
              - name: HF_HOME
                value: "/local/huggingface"
              - name: HF_HUB_VERBOSITY
                value: "debug"
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token-secret
                    key: token
              - name: NCCL_DEBUG
                value: "TRACE"
            command: ["/bin/bash"]
            args:
              - "-c"
              - |
                set -x
                # start ray worker
                /ray_init.sh worker --ray_address=$(LWS_LEADER_ADDRESS)
            resources:
              limits:
                nvidia.com/gpu: "4"
                memory: 64Gi
                ephemeral-storage: 200Gi
                vpc.amazonaws.com/efa: 1
              requests:
                cpu: 12
                ephemeral-storage: 200Gi
                vpc.amazonaws.com/efa: 1
            volumeMounts:
              - name: local-storage
                mountPath: /local
              - name: shm
                mountPath: /dev/shm
        volumes:
        - name: local-storage
          hostPath:
            path: /root/local
            type: DirectoryOrCreate
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-leader
spec:
  ports:
    - name: port-8000
      port: 8000
      targetPort: 8000
    - name: port-8265
      port: 8265
      targetPort: 8265
  type: ClusterIP
  selector:
    leaderworkerset.sigs.k8s.io/name: vllm
    role: leader
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token-secret
type: Opaque
data:
  token: "aGZfTUVzd1BVRlVZc2ZVTkRRRkVzTXF0UVBvbUx1bXZoVmdXQg=="
